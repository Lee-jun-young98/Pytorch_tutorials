{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Torchvision 객체검출 매세조정(Finetuning) 튜토리얼.ipynb","provenance":[],"authorship_tag":"ABX9TyPDANYzb7zrvZc6zcVNsnlB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P4UTaYr83fHJ","executionInfo":{"status":"ok","timestamp":1648007254849,"user_tz":-540,"elapsed":15800,"user":{"displayName":"이준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14894232288140235299"}},"outputId":"a2ccf8e5-6eed-450c-e2b7-39c8f74ca4b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.28)\n","Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n","  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-detc2c3s\n","  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-detc2c3s\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (57.4.0)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (0.29.28)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.4.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.0.7)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.21.5)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools==2.0) (3.10.0.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n","Building wheels for collected packages: pycocotools\n","  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycocotools: filename=pycocotools-2.0-cp37-cp37m-linux_x86_64.whl size=264358 sha256=65418eaff059fce0a38deb257eea40d3b226095cb452f9e929c1a52e20fe445f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-g08bb288/wheels/e2/6b/1d/344ac773c7495ea0b85eb228bc66daec7400a143a92d36b7b1\n","Successfully built pycocotools\n","Installing collected packages: pycocotools\n","  Attempting uninstall: pycocotools\n","    Found existing installation: pycocotools 2.0.4\n","    Uninstalling pycocotools-2.0.4:\n","      Successfully uninstalled pycocotools-2.0.4\n","Successfully installed pycocotools-2.0\n"]},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{},"execution_count":1}],"source":["%%shell\n","\n","pip install cython\n","pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"]},{"cell_type":"markdown","source":["# Penn-Fundan 다운로드"],"metadata":{"id":"2yyUK6wo4qeX"}},{"cell_type":"code","source":["%%shell\n","\n","# download the Penn-Fudan dataset\n","wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n","\n","# extract it in the current folder\n","unzip PennFudanPed.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4OdoPJm248Ln","outputId":"243db96d-c359-4392-e10e-19720b89b8d4"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["--2022-03-23 06:34:31--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n","Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\n","Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 53723336 (51M) [application/zip]\n","Saving to: ‘PennFudanPed.zip.1’\n","\n","PennFudanPed.zip.1  100%[===================>]  51.23M  30.5MB/s    in 1.7s    \n","\n","2022-03-23 06:34:33 (30.5 MB/s) - ‘PennFudanPed.zip.1’ saved [53723336/53723336]\n","\n","Archive:  PennFudanPed.zip\n","replace PennFudanPed/added-object-list.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"code","source":["from PIL import Image\n","Image.open('PennFudanPed/PNGImages/FudanPed00001.png')"],"metadata":{"id":"nhd88UM85KV2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mask = Image.open(\"PennFudanPed/PedMasks/FudanPed00001_mask.png\")"],"metadata":{"id":"ZCtyw2Oe-dwb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mask.putpalette([\n","                 0,0,0, # black background\n","                 255, 0, 0, # index 1은 red\n","                 255, 255, 0, # index 2는 yellow\n","                 255, 153, 0, # index 3은 오렌지 \n","])\n","mask"],"metadata":{"id":"P48rQ0lq-mDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import torch\n","import torch.utils.data\n","from PIL import Image\n","\n","\n","class PennFudanDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        # 모든 이미지와 피일 읽고 정렬\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n","\n","    def __getitem__(self, idx):\n","        # 이미지와 마스크 읽기\n","        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # img만 RGB로 변환\n","        mask = Image.open(mask_path)\n","\n","        # numpy 배열을 PIL이미지로 변환\n","        mask = np.array(mask)\n","        # 인스턴스들은 다른색으로 코딩되어 있음\n","        obj_ids = np.unique(mask)\n","        # 첫번째 id는 배경이라 제거\n","        obj_ids = obj_ids[1:]\n","\n","        # 컬러 인코딩된 마스크를 바이너리 마스크 세트로 나눔\n","        masks = mask == obj_ids[:, None, None]\n","\n","        # 각 마스크의 바운딩 박스 좌표를 얻음\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.where(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        # 모든 것을 torch tensor 좌표로 바꿈\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # 객체 종류는 한 종류만 존재(여기에서는 사람)\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # 모든 인스턴스는 군중 상태가 아님을 가정\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)"],"metadata":{"id":"HzdWITGR-0Dr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = PennFudanDataset('PennFudanPed/')\n","dataset[0]"],"metadata":{"id":"870oa5xeCnyE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 미리 학습된 모델로부터 미세 조정"],"metadata":{"id":"kq9h-CDQKiSw"}},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","# COCO로 미리 학습된 모델 읽기\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","# 분류기를 새로운 것으로 교체 (배경 + 사람) 2개의 클래스\n","num_classes = 2\n","\n","# 분류기에서 사용할 입력 특징의 차원 정보 얻기\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","# 미리 학습된 모델의 머리 부분을 새로운 것으로 교체\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"],"metadata":{"id":"dnMyC_9YFBQJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 다른 백본을 추가하도록 모델을 수정"],"metadata":{"id":"OPcF1GoQJSyn"}},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# 분류 목적으로 미리 학습된 모델을 로드하고 특징들만 리턴\n","backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n","\n","# Faster RCNN은 백본의 출력 채널 수를 알아야 함\n","# mobilenetV2의 경우 1280이므로 여기에 추가\n","backbone.out_channels = 1280\n","\n","# RPN이 5개의 서로 다른 크기와 3개의 다른 측면 비율을 가진 5 x 3개의 앵커를 공간 위치마다 생성\n","# 각 특징 맵이 잠재적으로 다른 사이즈와 측면 비율을 가질 수 있기 때문에 Tuple[Tuple[int]] 타입을 가지도록 함\n","\n","anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","                                   aspect_ratios =((0.5,1.0,2.0),))\n","\n","# 관심 영역의 자르기 및 재할당 후 자르기 크기를 수행하는데 사용할 피처맵 정의\n","# 백본이 텐서를 리턴할 때, featmap_names는 [0]이 될 것이라 예상\n","# 일반적으로 백본을 OrderedDict[Tensor] 타입을 리턴해야 함\n","# 특징맵에서 사용할 featmap_names 값을 정할 수 있음\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","# 조각들을 Faster RCNN 모델로 합침\n","model = FasterRCNN(backbone,\n","                   num_classes=2,\n","                   rpn_anchor_generator=anchor_generator,\n","                   box_roi_pool=roi_pooler)\n"],"metadata":{"id":"-Snsf-7uKmHC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PennFudan 데이터셋을 위한 인스턴스 분할 모델"],"metadata":{"id":"_wtPs9m-c6Vt"}},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","def get_model_instance_segmentation(num_classes):\n","    # COCO에서 미리 학습된 인스턴스 분할 모델 읽어오기\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained = True)\n","\n","    # 분류를 위한 입력 특징 차원 얻기\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # 미리 학습된 헤더를 새로운 것으로 바꿈\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # 마스크 분류기를 위한 입력 특징 차원 얻기\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","\n","    # 마스크 예측기를 새로운 것으로 바꿈\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n","\n","    return model"],"metadata":{"id":"lDa6dXGnc-R9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%shell\n","\n","# Download TorchVision repo to use some files from\n","# references/detection\n","git clone https://github.com/pytorch/vision.git\n","cd vision\n","git checkout v0.8.2"],"metadata":{"id":"RXGx2tg5SHyw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모든 것을 하나로 합치기"],"metadata":{"id":"Oyd9oaqRMgIx"}},{"cell_type":"code","source":["from engine import train_one_epoch, evaluate\n","import utils\n","import transforms as T\n","\n","def get_transform(train):\n","  transforms = []\n","  transforms.append(T.ToTensor())\n","  if train:\n","      transforms.append(T.RandomHorizontalFlip(0.5))\n","  return T.Compose(transforms)"],"metadata":{"id":"v5gDNePvRtYp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# forward() 메소드 테스트하기"],"metadata":{"id":"T7xWQ8bTVqUo"}},{"cell_type":"code","source":["model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size = 2, shuffle = True, num_workers = 4,\n","    collate_fn=utils.collate_fn)\n","\n","# 학습 시\n","images, targets = next(iter(data_loader))\n","images = list(image for image in images)\n","targets = [{k: v for k, v in t.items()} for t in targets]\n","output = model(images, targets)\n","\n","# 추론 시\n","model.eval()\n","x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","predictions = model(x)"],"metadata":{"id":"RqJoycZ9WLoe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from engine import train_one_epoch, evaluate\n","import utils\n","\n","def main():\n","  # 학습을 GPU로 진행하되 없으면 CPU로 진행\n","  device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","  # 배경과 사람 2개의 클래스\n","  num_classes = 2\n","\n","  # 데이터셋 정의된 변환\n","  dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","  dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n","\n","  # train과 test셋 나누기(전체 50개 테스트, 나머지 학습)\n","  indices = torch.randperm(len(dataset)).tolist()\n","  dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","  dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","  # 데이터 로더를 학습용과 검증용으로 정의\n","  data_loader = torch.utils.data.DataLoader(\n","      dataset, batch_size = 2, shuffle = True, num_workers = 4,\n","      collate_fn = utils.collate_fn\n","  )\n","  data_loader_test = torch.utils.data.DataLoader(\n","      dataset_test, batch_size = 1, shuffle = False, num_workers = 4,\n","      collate_fn = utils.collate_fn\n","  )\n","\n","  # 도움 함수를 이용해 모델 가져오기\n","  model = get_model_instance_segmentation(num_classes)\n","\n","  # 모델을 GPU나 CPU로 옮김\n","  model.to(device)\n","\n","  # 옵티마이저를 만듦\n","  params = [p for p in model.parameters() if p.requires_grad]\n","  optimizer = torch.optim.SGD(params, lr = 0.005,\n","                              momentum = 0.9, weight_decay = 0.0005)\n","  \n","  # 학습을 스케줄러를 만듦\n","  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.1)\n","\n","\n","  # 10 에포크만 학습\n","  num_epochs = 10\n","\n","\n","  for epoch in range(num_epochs):\n","    # 1 에포크학습 10회마다 출력\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq = 10)\n","    # 학습률 업데이트\n","    lr_scheduler.step()\n","    # 테스트 데이터셋 평가\n","    evaluate(model, data_loader_test, device = device)\n","\n","  print(\"That's it!\")\n"],"metadata":{"id":"Y8ayABJXW0-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"id":"SNlwEj_Ucuol"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ACcVVXybd7c1"},"execution_count":null,"outputs":[]}]}